{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Augmentations+\n",
    "\n",
    "This notebook provides the code for training a network with the following augmentation settings: <br>\n",
    "* <b>Geometric Augmentations</b> \n",
    "    - e.g., Horizontal Flip, Rotation, etc.\n",
    "* <b>Photometric Augmentations</b> \n",
    "    - e.g., Autocontrast, Equalization\n",
    "* <b>Downsampling</b> \n",
    "    - i.e., downsamples an image and then resizes it to its original size\n",
    "* <b>Common Corruptions</b> \n",
    "    - e.g., motion blur, frost, etc.\n",
    "* <b>Amplitude-Phase Recombination</b>\n",
    "    * APR-Single: Swaps the amplitude and phase info of an image and its augmented version in the freq. domain\n",
    "    * APR-Pair: Swaps the amplitude and phase info of two images in the freq. domain \n",
    "* <b>Amplitude-Adjust</b> \n",
    "    - i.e., changes the intensity of the amplitude info in the freq. domain\n",
    "<br><br>\n",
    "\n",
    "Note: \n",
    "- This code is heavily based on the Amplitude-Phase Recombination github.\n",
    "\n",
    "References:\n",
    "* Amplitude-Phase Recombination: https://github.com/iCGY96/APR\n",
    "* Common Corruptions: https://github.com/bethgelab/imagecorruptions\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from datasets import CIFAR10D, CIFAR100D, CustomDataset\n",
    "from utils.utils import AverageMeter, Logger, save_networks, load_networks\n",
    "from core import train, test_two_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Parameters\n",
    "\n",
    "Augmentations Notes:\n",
    "- none: no augmentations\n",
    "- normal: normal augmentations (geometric, photometric, geo-photo)\n",
    "- aprs: apr-single\n",
    "- aprp: apr-pair\n",
    "- amp-adj: amplitude adjust\n",
    "- com-cor: common corruptions\n",
    "- dsamp: downsampling\n",
    "- faa: faster autoaugment (requires policy weights inside the faster_autoaugment/policy_weights)\n",
    "\n",
    "The augmentation be selected using a list in the main_aug. > <b>[(*AUG1, *PROB), (*AUG2, *PROB)]</b> <br>\n",
    "The augmentations are implemented one by one according to their arrangement.\n",
    "\n",
    "The parameters usually modified from experiment to experiment has <u><b>## USUALLY MODIFIED:</b></u> comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {}\n",
    "\n",
    "# dataset\n",
    "options['data'] = './data'\n",
    "options['outf'] = './results'\n",
    "options['dataset'] = 'mosquito'  ## USUALLY MODIFIED: 'cifar10' or 'mosquito'\n",
    "options['dataset_mosq'] = 'HQ100' ## when mosquito dataset is selected, choose dataset ratio ('HQ100': High Quality 100%, 'HQ100LQ20': HQ 100% and LQ 20%)\n",
    "options['workers'] = 2 ## number of data loading workers (default: 2)\n",
    "options['input_size'] = 224 ## USUALLY MODIFIED\n",
    "options['dataset_addr_HQ100'] = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-high/train/',   ## using train folder\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/raw-hl/low/',\n",
    "        }\n",
    "options['dataset_addr_HQ100LQ20'] = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-comb/100-20/train/',   ## using combined train folder (HQ100% - LQ20%)\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/sets/raw-low/test/',\n",
    "        }\n",
    "\n",
    "\n",
    "## AUGMENTATIONS \n",
    "## USUALLY MODIFIED: main augmentation ['aprs', 'normal', 'com-cor', 'amp-adj', 'dsamp', 'faa'] DEFAULT: []\n",
    "## it has tuples to take in the probability of the augmentation e.g. [(*AUG, *PROB)]\n",
    "options['main_aug'] = [('faa', .5)]# [('aprs', .5), ('amp-adj', .2), ('com-cor', .5)]                        \n",
    "options['aprp'] = False ## APR-Pair is activated in the training, turn on/off here\n",
    "## when the normal augmentations is activated in main_aug, it will use aug_set for the kind of augmentations to use\n",
    "options['aug_set'] = 'geo-photo' ## ['geo', 'photo', 'geo-photo', 'geo-k', 'photo-k', 'geo-photo-k'] '-k' means kornia version or the default PIL ver\n",
    "options['faa_policy_addr'] = 'datasets/faster_autoaugment/policy_weights/19_dsaa.pt' ## used when the faa policy is activated\n",
    "\n",
    "\n",
    "# optimization\n",
    "options['batch_size'] = 32 ## USUALLY MODIFIED\n",
    "options['lr'] = 0.1 ## model learning rate\n",
    "options['max_epoch'] = 200\n",
    "options['stepsize'] = 30\n",
    "\n",
    "# model\n",
    "options['model'] = 'resnet18' ## ['resnet18', 'wide_resnet', allconv, 'densenet', 'resnext']\n",
    "# load model parameters\n",
    "options['load_network'] = False ## if True, the model parameters and criterion from files below will be loaded\n",
    "load_network_adr = \"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj_.pth\" ## address of the model parameters to load\n",
    "load_criterion_adr = \"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj__criterion.pth\"\n",
    "\n",
    "# misc\n",
    "options['eval_freq'] = 10  ## it will print results every eval_freq epochs\n",
    "options['gpu'] = '0'\n",
    "options['seed'] = 0\n",
    "options['use_cpu'] = False\n",
    "options['eval'] = False ## train or evaluate\n",
    "\n",
    "if not os.path.exists(options['outf']):\n",
    "    os.makedirs(options['outf'])\n",
    "\n",
    "if not os.path.exists(options['data']):\n",
    "    os.makedirs(options['data'])\n",
    "\n",
    "# misc 2\n",
    "options['outf'] = \"None\" ## USUALLY MODIFIED: checkpoint address [\"./results/checkpoints/NAMEOFEXPERIMENT/\", \"None\"]\n",
    "options['actual_print'] = 4  ## number of actual print frequency (i.e., the number of loss values shown per epoch and options['eval_freq'])\n",
    "\n",
    "\n",
    "if options['outf'] == \"None\":\n",
    "    options['outf'] = \"./results/DefaultBin/\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the seed and use GPU when available unless explicitly set to CPU in the options above\n",
    "torch.manual_seed(options['seed'])\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if options['use_cpu']: use_gpu = False\n",
    "\n",
    "options.update({'use_gpu': use_gpu})\n",
    "\n",
    "if use_gpu:\n",
    "    print(\"Currently using GPU: {}\".format(options['gpu']))\n",
    "    cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(options['seed'])\n",
    "else:\n",
    "    print(\"Currently using CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up the dataset to use\n",
    "- Train Set: Training Dataset\n",
    "- Test Set: Test Dataset \n",
    "- Out Set: A separate Test Dataset\n",
    "\n",
    "The network is trained on the training set and tested on the test set and out set.<br>\n",
    "Both the test set and out set are not seen or touched by the network.<br>\n",
    "\n",
    "The out set is normally used as another dataset from another distribution (e.g., low quality images of the same class as the test set).<br>\n",
    "It tests if the network can also perform well on these datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset using the dataset address\n",
    "\n",
    "if options['dataset'] == 'cifar10':\n",
    "    Data = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "elif options['dataset'] == 'mosquito': ## for mosquito dataset\n",
    "    if options['dataset_mosq'] == 'HQ100':\n",
    "        data_dir = options['dataset_addr_HQ100']\n",
    "\n",
    "    elif options['dataset_mosq'] == 'HQ100LQ20':\n",
    "        data_dir = options['dataset_addr_HQ100LQ20']\n",
    "        \n",
    "    Data = CustomDataset(dataroot=data_dir, batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=True, input_size=options['input_size'], faa_policy=options['faa_policy_addr'])\n",
    "\n",
    "    ## Initialize the dataloader\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loaders\n",
    "\n",
    "else: ## for CIFAR100 dataset\n",
    "    Data = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "\n",
    "num_classes = Data.num_classes\n",
    "\n",
    "## modify the print frequency based on the trainloader\n",
    "options['print_freq'] = int(len(trainloader)/(options['actual_print']))\n",
    "\n",
    "print(\"Creating model: {}\".format(options['model']))\n",
    "if 'wide_resnet' in options['model']:\n",
    "    print('wide_resnet')\n",
    "    from model.wide_resnet import WideResNet\n",
    "    net = WideResNet(40, num_classes, 2, 0.0)\n",
    "elif 'allconv' in options['model']:\n",
    "    print('allconv')\n",
    "    from model.allconv import AllConvNet\n",
    "    net = AllConvNet(num_classes)\n",
    "elif 'densenet' in options['model']:\n",
    "    print('densenet')\n",
    "    from model.densenet import  densenet\n",
    "    net = densenet(num_classes=num_classes)\n",
    "elif 'resnext' in options['model']:\n",
    "    print('resnext29')\n",
    "    from model.resnext import resnext29\n",
    "    net = resnext29(num_classes)\n",
    "else:\n",
    "    print('resnet18')\n",
    "    from model.resnet import ResNet18\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "if use_gpu:\n",
    "    net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['main_aug'])\n",
    "\n",
    "if options['load_network']:\n",
    "    ## reload last saved network\n",
    "    net.load_state_dict(torch.load(load_network_adr))\n",
    "    criterion.load_state_dict(torch.load(load_criterion_adr))\n",
    "\n",
    "\n",
    "params_list = [{'params': net.parameters()},\n",
    "            {'params': criterion.parameters()}]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_acc, best_acc_out = 0.0, 0.0\n",
    "for epoch in range(options['max_epoch']):\n",
    "    print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
    "\n",
    "    ## Train the network\n",
    "    train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
    "\n",
    "    ## Print the results and save the best parameters for dataset and outset\n",
    "    if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch']:\n",
    "        print(\"==> Test\")\n",
    "        results = test_two_datasets(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
    "\n",
    "        if best_acc < results['ACC']:\n",
    "            best_acc = results['ACC']\n",
    "            print(\"Best Test Set Acc (%): {:.3f}\\t\".format(best_acc))\n",
    "            ## save the parameters for the best acc\n",
    "            save_networks(net, options['outf'], file_name, loss='BestAcc', criterion=criterion)\n",
    "\n",
    "        if best_acc_out < results['ACC_OUT']:\n",
    "            best_acc_out = results['ACC_OUT']\n",
    "            print(\"Best Out Set Acc (%): {:.3f}\\t\".format(best_acc_out))\n",
    "            ## save the parameters for the best acc out\n",
    "            save_networks(net, options['outf'], file_name, loss='BestAccOut', criterion=criterion)\n",
    "        \n",
    "        save_networks(net, options['outf'], file_name, loss='LastEpoch', criterion=criterion)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "elapsed = round(time.time() - start_time)\n",
    "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apr-kornia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
