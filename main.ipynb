{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Augmentations+\n",
    "\n",
    "This notebook provides the code for training a network with the following augmentation settings: <br>\n",
    "* Geometric Augmentations (e.g., Horizontal Flip, Rotation, etc.)\n",
    "* Photometric Augmentations (e.g., Autocontrast, Equalization)\n",
    "* Downsampling\n",
    "* Common Corruptions\n",
    "* Amplitude-Phase Recombination\n",
    "<br><br>\n",
    "\n",
    "Note: \n",
    "- This code is heavily based on the Amplitude-Phase Recombination github.\n",
    "\n",
    "References:\n",
    "* Amplitude-Phase Recombination: https://github.com/iCGY96/APR\n",
    "* Common Corruptions: https://github.com/bethgelab/imagecorruptions\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import importlib\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from datasets import CIFAR10D, CIFAR100D, CustomDataset\n",
    "from utils.utils import AverageMeter, Logger, save_networks, load_networks\n",
    "from core import train, test, test_robustness, test_two_datasets\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Training\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"Training\")\n",
    "options = {}\n",
    "\n",
    "# dataset\n",
    "options['data'] = './data'\n",
    "options['outf'] = './results'\n",
    "options['dataset'] = 'mosquito'  ## USUALLY MODIFIED: 'cifar10' or 'mosquito'\n",
    "options['dataset_mosq'] = 'HQ100' ## when mosquito dataset is selected, choose dataset ratio ('HQ100': High Quality 100%, 'HQ100LQ20': HQ 100% and LQ 20%)\n",
    "options['workers'] = 8 ## number of data loading workers (default: 2)\n",
    "options['input_size'] = 224 ## USUALLY MODIFIED\n",
    "\n",
    "# optimization\n",
    "options['batch_size'] = 32 ## USUALLY MODIFIED\n",
    "options['lr'] = 0.1 ## model learning rate\n",
    "options['max_epoch'] = 200\n",
    "options['stepsize'] = 30\n",
    "options['aug'] = 'aprs' ## USUALLY MODIFIED: ['none', 'aprs', 'new-aug', 'amp-adj']\n",
    "\n",
    "# model\n",
    "options['model'] = 'resnet18' ## ['resnet18', 'wide_resnet', allconv, 'densenet', 'resnext']\n",
    "\n",
    "# misc\n",
    "options['eval_freq'] = 10\n",
    "options['print_freq'] = 100\n",
    "options['gpu'] = '0'\n",
    "options['seed'] = 0\n",
    "options['use_cpu'] = True\n",
    "options['eval'] = False ## train or evaluate\n",
    "\n",
    "# parameters for generating adversarial examples\n",
    "options['epsilon'] = 0.0157 ## maximum perturbation of adversaries (4/255=0.0157)\n",
    "options['alpha'] = 0.00784 ## movement multiplier per iteration when generating adversarial examples (2/255=0.00784)\n",
    "options['k'] = 10 ## maximum iteration when generating adversarial examples\n",
    "options['perturbation_type'] = 'linf' ## the type of the perturbation ('linf' or 'l2')\n",
    "\n",
    "\n",
    "if not os.path.exists(options['outf']):\n",
    "    os.makedirs(options['outf'])\n",
    "\n",
    "if not os.path.exists(options['data']):\n",
    "    os.makedirs(options['data'])\n",
    "\n",
    "# misc 2\n",
    "options['outf'] = \"None\" ## USUALLY MODIFIED: checkpoint address [\"./results/checkpoints/NAMEOFEXPERIMENT/\", \"None\"]\n",
    "options['actual_print'] = 4  ## number of actual print frequency (i.e., the number of loss values shown per epoch)\n",
    "\n",
    "\n",
    "if options['outf'] == \"None\":\n",
    "    options['outf'] = \"./results/checkpoints/DefaultBin/\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using CPU\n"
     ]
    }
   ],
   "source": [
    "## Set the seed and use GPU when available unless explicitly set to CPU in the options above\n",
    "torch.manual_seed(options['seed'])\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if options['use_cpu']: use_gpu = False\n",
    "\n",
    "options.update({'use_gpu': use_gpu})\n",
    "\n",
    "if use_gpu:\n",
    "    print(\"Currently using GPU: {}\".format(options['gpu']))\n",
    "    cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(options['seed'])\n",
    "else:\n",
    "    print(\"Currently using CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up the dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APRecombination aprs\n",
      "Creating model: resnet18\n",
      "resnet18\n",
      "==> Epoch 1/200\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m    101\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==> Epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m--> 103\u001b[0m     train(net, criterion, optimizer, trainloader, epoch\u001b[39m=\u001b[39;49mepoch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m options[\u001b[39m'\u001b[39m\u001b[39meval_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m options[\u001b[39m'\u001b[39m\u001b[39meval_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mor\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m    106\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==> Test\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\PostDoc\\Coding\\Z-Turnover\\NormalAugmentations_Plus\\core\\train.py:81\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, criterion, optimizer, trainloader, epoch, **options)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     inputs, targets \u001b[39m=\u001b[39m data, labels\n\u001b[1;32m---> 81\u001b[0m inputs_mix \u001b[39m=\u001b[39m mix_data(inputs)\n\u001b[0;32m     82\u001b[0m inputs_mix \u001b[39m=\u001b[39m Variable(inputs_mix)\n\u001b[0;32m     83\u001b[0m batch_size \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32me:\\PostDoc\\Coding\\Z-Turnover\\NormalAugmentations_Plus\\core\\train.py:28\u001b[0m, in \u001b[0;36mmix_data\u001b[1;34m(x, use_cuda, prob)\u001b[0m\n\u001b[0;32m     25\u001b[0m fft_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfft\u001b[39m.\u001b[39mfftn(x, dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[0;32m     26\u001b[0m abs_1, angle_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(fft_1), torch\u001b[39m.\u001b[39mangle(fft_1)\n\u001b[1;32m---> 28\u001b[0m fft_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfft\u001b[39m.\u001b[39mfftn(x[index, :], dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[0;32m     29\u001b[0m abs_2, angle_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(fft_2), torch\u001b[39m.\u001b[39mangle(fft_2)\n\u001b[0;32m     31\u001b[0m fft_1 \u001b[39m=\u001b[39m abs_2\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mexp((\u001b[39m1\u001b[39mj) \u001b[39m*\u001b[39m angle_1)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "if options['dataset'] == 'cifar10':\n",
    "    Data = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "elif options['dataset'] == 'mosquito': ## for mosquito dataset\n",
    "    if options['dataset_mosq'] == 'HQ100':\n",
    "        data_dir = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-high/train/',   ## using train folder\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/raw-hl/low/',\n",
    "        }\n",
    "\n",
    "    elif options['dataset_mosq'] == 'HQ100LQ20':\n",
    "        data_dir = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-comb/100-20/train/',   ## using combined train folder (HQ100% - LQ20%)\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/sets/raw-low/test/',\n",
    "\n",
    "        }\n",
    "    Data = CustomDataset(dataroot=data_dir, batch_size=options['batch_size'], _transforms=options['aug'], _eval=True, input_size=options['input_size'])\n",
    "\n",
    "    ## Initialize the dataloader\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loaders\n",
    "\n",
    "else: ## for CIFAR100 dataset\n",
    "    Data = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "\n",
    "num_classes = Data.num_classes\n",
    "\n",
    "## modify the print frequency based on the trainloader\n",
    "options['print_freq'] = int(len(trainloader)/(options['actual_print']))\n",
    "\n",
    "print(\"Creating model: {}\".format(options['model']))\n",
    "if 'wide_resnet' in options['model']:\n",
    "    print('wide_resnet')\n",
    "    from model.wide_resnet import WideResNet\n",
    "    net = WideResNet(40, num_classes, 2, 0.0)\n",
    "elif 'allconv' in options['model']:\n",
    "    print('allconv')\n",
    "    from model.allconv import AllConvNet\n",
    "    net = AllConvNet(num_classes)\n",
    "elif 'densenet' in options['model']:\n",
    "    print('densenet')\n",
    "    from model.densenet import  densenet\n",
    "    net = densenet(num_classes=num_classes)\n",
    "elif 'resnext' in options['model']:\n",
    "    print('resnext29')\n",
    "    from model.resnext import resnext29\n",
    "    net = resnext29(num_classes)\n",
    "else:\n",
    "    print('resnet18')\n",
    "    from model.resnet import ResNet18\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "if use_gpu:\n",
    "    net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['aug'])\n",
    "\n",
    "## reload last saved network\n",
    "#net.load_state_dict(torch.load(\"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj_.pth\"))\n",
    "#criterion.load_state_dict(torch.load(\"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj__criterion.pth\"))\n",
    "\n",
    "if options['eval']:\n",
    "    net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n",
    "    outloaders = Data.out_loaders\n",
    "    results = test(net, criterion, testloader, outloader, epoch=0, **options)\n",
    "    acc = results['ACC']\n",
    "    res = dict()\n",
    "    res['ACC'] = dict()\n",
    "    acc_res = []\n",
    "    for key in Data.out_keys:\n",
    "        results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n",
    "        print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n",
    "        res['ACC'][key] = results['ACC']\n",
    "        acc_res.append(results['ACC'])\n",
    "    print('Mean ACC:', np.mean(acc_res))\n",
    "    print('Mean Error:', 100-np.mean(acc_res))\n",
    "\n",
    "\n",
    "\n",
    "params_list = [{'params': net.parameters()},\n",
    "            {'params': criterion.parameters()}]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(options['max_epoch']):\n",
    "    print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
    "\n",
    "    train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
    "\n",
    "    if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch'] or epoch > 200:\n",
    "        print(\"==> Test\")\n",
    "        results = test_two_datasets(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
    "\n",
    "        if best_acc < results['ACC']:\n",
    "            best_acc = results['ACC']\n",
    "            print(\"Best Acc (%): {:.3f}\\t\".format(best_acc))\n",
    "        \n",
    "        save_networks(net, options['outf'], file_name, criterion=criterion)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "elapsed = round(time.time() - start_time)\n",
    "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apr-kornia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
