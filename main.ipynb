{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Augmentations+\n",
    "\n",
    "This notebook provides the code for training a network with the following augmentation settings: <br>\n",
    "* Geometric Augmentations (e.g., Horizontal Flip, Rotation, etc.)\n",
    "* Photometric Augmentations (e.g., Autocontrast, Equalization)\n",
    "* Downsampling\n",
    "* Common Corruptions\n",
    "* Amplitude-Phase Recombination\n",
    "* Amplitude-Adjust (changes the intensity of the amplitude info in the freq. domain)\n",
    "<br><br>\n",
    "\n",
    "Note: \n",
    "- This code is heavily based on the Amplitude-Phase Recombination github.\n",
    "\n",
    "References:\n",
    "* Amplitude-Phase Recombination: https://github.com/iCGY96/APR\n",
    "* Common Corruptions: https://github.com/bethgelab/imagecorruptions\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from datasets import CIFAR10D, CIFAR100D, CustomDataset\n",
    "from utils.utils import AverageMeter, Logger, save_networks, load_networks\n",
    "from core import train, test, test_robustness, test_two_datasets\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Training\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Parameters\n",
    "\n",
    "Augmentations Notes:\n",
    "- none: no augmentations\n",
    "- normal: normal augmentations (geometric, photometric, geo-photo)\n",
    "- com-cor: common corruptions\n",
    "- downsmp: downsampling\n",
    "- faa: faster autoaugment (requires policy weights inside the faster_autoaugment/policy_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"Training\")\n",
    "options = {}\n",
    "\n",
    "# dataset\n",
    "options['data'] = './data'\n",
    "options['outf'] = './results'\n",
    "options['dataset'] = 'mosquito'  ## USUALLY MODIFIED: 'cifar10' or 'mosquito'\n",
    "options['dataset_mosq'] = 'HQ100' ## when mosquito dataset is selected, choose dataset ratio ('HQ100': High Quality 100%, 'HQ100LQ20': HQ 100% and LQ 20%)\n",
    "options['workers'] = 2 ## number of data loading workers (default: 2)\n",
    "options['input_size'] = 224 ## USUALLY MODIFIED\n",
    "options['dataset_addr_HQ100'] = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-high/train/',   ## using train folder\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/raw-hl/low/',\n",
    "        }\n",
    "options['dataset_addr_HQ100LQ20'] = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-comb/100-20/train/',   ## using combined train folder (HQ100% - LQ20%)\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/sets/raw-low/test/',\n",
    "        }\n",
    "\n",
    "\n",
    "## AUGMENTATIONS \n",
    "## USUALLY MODIFIED: main augmentation ['aprs', 'normal', 'com-cor', 'amp-adj', 'dsamp', 'faa'] DEFAULT: []\n",
    "## it has tuples to take in the probability of the augmentation\n",
    "options['main_aug'] = [('normal', .5)]# [('aprs', .5), ('amp-adj', .2), ('com-cor', .5)]                        \n",
    "options['aprp'] = False ## APR-Pair is activated in the training, turn on/off here\n",
    "## when the normal augmentations is activated in main_aug, it will use aug_set for the kind of augmentations to use\n",
    "options['aug_set'] = 'geo-photo' ## ['geo', 'photo', 'geo-photo', 'geo-k', 'photo-k', 'geo-photo-k'] '-k' means kornia version or the default PIL ver\n",
    "\n",
    "\n",
    "# optimization\n",
    "options['batch_size'] = 32 ## USUALLY MODIFIED\n",
    "options['lr'] = 0.1 ## model learning rate\n",
    "options['max_epoch'] = 200\n",
    "options['stepsize'] = 30\n",
    "\n",
    "# model\n",
    "options['model'] = 'resnet18' ## ['resnet18', 'wide_resnet', allconv, 'densenet', 'resnext']\n",
    "# load model parameters\n",
    "options['load_network'] = False ## if True, the model parameters and criterion from files below will be loaded\n",
    "load_network_adr = \"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj_.pth\" ## address of the model parameters to load\n",
    "load_criterion_adr = \"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj__criterion.pth\"\n",
    "\n",
    "# misc\n",
    "options['eval_freq'] = 10  ## it will print results every eval_freq epochs\n",
    "options['gpu'] = '0'\n",
    "options['seed'] = 0\n",
    "options['use_cpu'] = False\n",
    "options['eval'] = False ## train or evaluate\n",
    "\n",
    "if not os.path.exists(options['outf']):\n",
    "    os.makedirs(options['outf'])\n",
    "\n",
    "if not os.path.exists(options['data']):\n",
    "    os.makedirs(options['data'])\n",
    "\n",
    "# misc 2\n",
    "options['outf'] = \"None\" ## USUALLY MODIFIED: checkpoint address [\"./results/checkpoints/NAMEOFEXPERIMENT/\", \"None\"]\n",
    "options['actual_print'] = 4  ## number of actual print frequency (i.e., the number of loss values shown per epoch and options['eval_freq'])\n",
    "\n",
    "\n",
    "if options['outf'] == \"None\":\n",
    "    options['outf'] = \"./results/DefaultBin/\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using GPU: 0\n"
     ]
    }
   ],
   "source": [
    "## Set the seed and use GPU when available unless explicitly set to CPU in the options above\n",
    "torch.manual_seed(options['seed'])\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if options['use_cpu']: use_gpu = False\n",
    "\n",
    "options.update({'use_gpu': use_gpu})\n",
    "\n",
    "if use_gpu:\n",
    "    print(\"Currently using GPU: {}\".format(options['gpu']))\n",
    "    cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(options['seed'])\n",
    "else:\n",
    "    print(\"Currently using CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up the dataset to use\n",
    "- Train Set: Training Dataset\n",
    "- Test Set: Test Dataset \n",
    "- Out Set: A separate Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormalAugmentations ('normal', 0.5)\n",
      "Creating model: resnet18\n",
      "resnet18\n",
      "==> Epoch 1/200\n",
      "Batch 28/115\t Loss 1.904637 (4.331332)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==> Epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m---> 87\u001b[0m     train(net, criterion, optimizer, trainloader, epoch\u001b[39m=\u001b[39;49mepoch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m options[\u001b[39m'\u001b[39m\u001b[39meval_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m options[\u001b[39m'\u001b[39m\u001b[39meval_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     90\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==> Test\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\PostDoc\\Coding\\Z-Turnover\\NormalAugmentations_Plus\\core\\train.py:102\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, criterion, optimizer, trainloader, epoch, **options)\u001b[0m\n\u001b[0;32m     99\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    100\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m--> 102\u001b[0m losses\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39mitem(), targets\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39m))\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m (batch_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m options[\u001b[39m'\u001b[39m\u001b[39mprint_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    105\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[0;32m    106\u001b[0m           \u001b[39m.\u001b[39mformat(batch_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(trainloader), losses\u001b[39m.\u001b[39mval, losses\u001b[39m.\u001b[39mavg))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if options['dataset'] == 'cifar10':\n",
    "    Data = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "elif options['dataset'] == 'mosquito': ## for mosquito dataset\n",
    "    if options['dataset_mosq'] == 'HQ100':\n",
    "        data_dir = options['dataset_addr_HQ100']\n",
    "\n",
    "    elif options['dataset_mosq'] == 'HQ100LQ20':\n",
    "        data_dir = options['dataset_addr_HQ100LQ20']\n",
    "        \n",
    "    Data = CustomDataset(dataroot=data_dir, batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=True, input_size=options['input_size'])\n",
    "\n",
    "    ## Initialize the dataloader\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loaders\n",
    "\n",
    "else: ## for CIFAR100 dataset\n",
    "    Data = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "\n",
    "num_classes = Data.num_classes\n",
    "\n",
    "## modify the print frequency based on the trainloader\n",
    "options['print_freq'] = int(len(trainloader)/(options['actual_print']))\n",
    "\n",
    "print(\"Creating model: {}\".format(options['model']))\n",
    "if 'wide_resnet' in options['model']:\n",
    "    print('wide_resnet')\n",
    "    from model.wide_resnet import WideResNet\n",
    "    net = WideResNet(40, num_classes, 2, 0.0)\n",
    "elif 'allconv' in options['model']:\n",
    "    print('allconv')\n",
    "    from model.allconv import AllConvNet\n",
    "    net = AllConvNet(num_classes)\n",
    "elif 'densenet' in options['model']:\n",
    "    print('densenet')\n",
    "    from model.densenet import  densenet\n",
    "    net = densenet(num_classes=num_classes)\n",
    "elif 'resnext' in options['model']:\n",
    "    print('resnext29')\n",
    "    from model.resnext import resnext29\n",
    "    net = resnext29(num_classes)\n",
    "else:\n",
    "    print('resnet18')\n",
    "    from model.resnet import ResNet18\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "if use_gpu:\n",
    "    net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['main_aug'])\n",
    "\n",
    "if options['load_network']:\n",
    "    ## reload last saved network\n",
    "    net.load_state_dict(torch.load(load_network_adr))\n",
    "    criterion.load_state_dict(torch.load(load_criterion_adr))\n",
    "\n",
    "\n",
    "params_list = [{'params': net.parameters()},\n",
    "            {'params': criterion.parameters()}]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_acc, best_acc_out = 0.0, 0.0\n",
    "for epoch in range(options['max_epoch']):\n",
    "    print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
    "\n",
    "    train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
    "\n",
    "    if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch']:\n",
    "        print(\"==> Test\")\n",
    "        results = test_two_datasets(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
    "\n",
    "        if best_acc < results['ACC']:\n",
    "            best_acc = results['ACC']\n",
    "            print(\"Best Test Set Acc (%): {:.3f}\\t\".format(best_acc))\n",
    "            ## save the parameters for the best acc\n",
    "            save_networks(net, options['outf'], file_name, loss='BestAcc', criterion=criterion)\n",
    "\n",
    "        if best_acc_out < results['ACC_OUT']:\n",
    "            best_acc_out = results['ACC_OUT']\n",
    "            print(\"Best Out Set Acc (%): {:.3f}\\t\".format(best_acc_out))\n",
    "            ## save the parameters for the best acc out\n",
    "            save_networks(net, options['outf'], file_name, loss='BestAccOut', criterion=criterion)\n",
    "        \n",
    "        save_networks(net, options['outf'], file_name, loss='LastEpoch', criterion=criterion)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "elapsed = round(time.time() - start_time)\n",
    "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apr-kornia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
