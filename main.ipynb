{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Augmentations+\n",
    "\n",
    "This notebook provides the code for training a network with the following augmentation settings: <br>\n",
    "* Geometric Augmentations (e.g., Horizontal Flip, Rotation, etc.)\n",
    "* Photometric Augmentations (e.g., Autocontrast, Equalization)\n",
    "* Downsampling\n",
    "* Common Corruptions\n",
    "* Amplitude-Phase Recombination\n",
    "* Amplitude-Adjust (changes the intensity of the amplitude info in the freq. domain)\n",
    "<br><br>\n",
    "\n",
    "Note: \n",
    "- This code is heavily based on the Amplitude-Phase Recombination github.\n",
    "\n",
    "References:\n",
    "* Amplitude-Phase Recombination: https://github.com/iCGY96/APR\n",
    "* Common Corruptions: https://github.com/bethgelab/imagecorruptions\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import importlib\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from datasets import CIFAR10D, CIFAR100D, CustomDataset\n",
    "from utils.utils import AverageMeter, Logger, save_networks, load_networks\n",
    "from core import train, test, test_robustness, test_two_datasets\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Training\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Parameters\n",
    "\n",
    "Augmentations Notes:\n",
    "- none: no augmentations\n",
    "- normal: normal augmentations (geo, geo-photo, photo)\n",
    "- com-cor: common corruptions\n",
    "- downsmp: downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"Training\")\n",
    "options = {}\n",
    "\n",
    "# dataset\n",
    "options['data'] = './data'\n",
    "options['outf'] = './results'\n",
    "options['dataset'] = 'mosquito'  ## USUALLY MODIFIED: 'cifar10' or 'mosquito'\n",
    "options['dataset_mosq'] = 'HQ100' ## when mosquito dataset is selected, choose dataset ratio ('HQ100': High Quality 100%, 'HQ100LQ20': HQ 100% and LQ 20%)\n",
    "options['workers'] = 8 ## number of data loading workers (default: 2)\n",
    "options['input_size'] = 224 ## USUALLY MODIFIED\n",
    "\n",
    "## AUGMENTATIONS \n",
    "## USUALLY MODIFIED: main augmentation ['aprs', 'normal', 'com-cor', 'amp-adj', 'dsamp'] DEFAULT: []\n",
    "## it has tuples to take in the probability of the augmentation\n",
    "options['main_aug'] = [('aprs', .5), ('amp-adj', .2), ('com-cor', .5)]                        \n",
    "options['aprp'] = False ## APR-Pair is activated in the training, turn on/off here\n",
    "options['aug_set'] = 'geo-photo' ## ['geo', 'photo', 'geo-photo', 'geo-k', 'photo-k', 'geo-photo-k'] '-k' means kornia version or the default PIL ver\n",
    "\n",
    "\n",
    "# optimization\n",
    "options['batch_size'] = 32 ## USUALLY MODIFIED\n",
    "options['lr'] = 0.1 ## model learning rate\n",
    "options['max_epoch'] = 2#00\n",
    "options['stepsize'] = 30\n",
    "\n",
    "# model\n",
    "options['model'] = 'resnet18' ## ['resnet18', 'wide_resnet', allconv, 'densenet', 'resnext']\n",
    "# load model parameters\n",
    "options['load_network'] = False ## if True, the model parameters and criterion from files below will be loaded\n",
    "load_network_adr = \"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj_.pth\" ## address of the model parameters to load\n",
    "load_criterion_adr = \"results/checkpoints/3_27_exp13/wider_resnet_28_10_mosquito_amp-adj__criterion.pth\"\n",
    "\n",
    "# misc\n",
    "options['eval_freq'] = 1#0\n",
    "options['print_freq'] = 100\n",
    "options['gpu'] = '0'\n",
    "options['seed'] = 0\n",
    "options['use_cpu'] = False\n",
    "options['eval'] = False ## train or evaluate\n",
    "\n",
    "# parameters for generating adversarial examples\n",
    "options['epsilon'] = 0.0157 ## maximum perturbation of adversaries (4/255=0.0157)\n",
    "options['alpha'] = 0.00784 ## movement multiplier per iteration when generating adversarial examples (2/255=0.00784)\n",
    "options['k'] = 10 ## maximum iteration when generating adversarial examples\n",
    "options['perturbation_type'] = 'linf' ## the type of the perturbation ('linf' or 'l2')\n",
    "\n",
    "\n",
    "if not os.path.exists(options['outf']):\n",
    "    os.makedirs(options['outf'])\n",
    "\n",
    "if not os.path.exists(options['data']):\n",
    "    os.makedirs(options['data'])\n",
    "\n",
    "# misc 2\n",
    "options['outf'] = \"None\" ## USUALLY MODIFIED: checkpoint address [\"./results/checkpoints/NAMEOFEXPERIMENT/\", \"None\"]\n",
    "options['actual_print'] = 4  ## number of actual print frequency (i.e., the number of loss values shown per epoch and options['eval_freq'])\n",
    "\n",
    "\n",
    "if options['outf'] == \"None\":\n",
    "    options['outf'] = \"./results/checkpoints/DefaultBin/\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using GPU: 0\n"
     ]
    }
   ],
   "source": [
    "## Set the seed and use GPU when available unless explicitly set to CPU in the options above\n",
    "torch.manual_seed(options['seed'])\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if options['use_cpu']: use_gpu = False\n",
    "\n",
    "options.update({'use_gpu': use_gpu})\n",
    "\n",
    "if use_gpu:\n",
    "    print(\"Currently using GPU: {}\".format(options['gpu']))\n",
    "    cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(options['seed'])\n",
    "else:\n",
    "    print(\"Currently using CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up the dataset to use\n",
    "- Train Set: Training Dataset\n",
    "- Test Set: Test Dataset \n",
    "- Out Set: A separate Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APRecombination ('aprs', 0.5)\n",
      "AMPAdjust ('amp-adj', 0.2)\n",
      "ComCorAugmentations ('com-cor', 0.5)\n",
      "Creating model: resnet18\n",
      "resnet18\n",
      "==> Epoch 1/2\n",
      "Batch 28/115\t Loss 2.522288 (2.990780)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==> Epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m---> 87\u001b[0m     train(net, criterion, optimizer, trainloader, epoch\u001b[39m=\u001b[39;49mepoch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m options[\u001b[39m'\u001b[39m\u001b[39meval_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m options[\u001b[39m'\u001b[39m\u001b[39meval_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m options[\u001b[39m'\u001b[39m\u001b[39mmax_epoch\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     90\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==> Test\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\PostDoc\\Coding\\Z-Turnover\\NormalAugmentations_Plus\\core\\train.py:75\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, criterion, optimizer, trainloader, epoch, **options)\u001b[0m\n\u001b[0;32m     72\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m     74\u001b[0m loss_all \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 75\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[0;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m options[\u001b[39m'\u001b[39m\u001b[39muse_gpu\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     77\u001b[0m         inputs, targets \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcuda(), labels\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\apr-kornia\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\apr-kornia\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\apr-kornia\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1284\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1285\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1286\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1287\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\apr-kornia\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1134\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\apr-kornia\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    178\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[0;32m    180\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\apr-kornia\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if options['dataset'] == 'cifar10':\n",
    "    Data = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "elif options['dataset'] == 'mosquito': ## for mosquito dataset\n",
    "    if options['dataset_mosq'] == 'HQ100':\n",
    "        data_dir = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-high/train/',   ## using train folder\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/raw-hl/low/',\n",
    "        }\n",
    "\n",
    "    elif options['dataset_mosq'] == 'HQ100LQ20':\n",
    "        data_dir = {\n",
    "            'train':'R:/Datasets/mosquito/sets/raw-comb/100-20/train/',   ## using combined train folder (HQ100% - LQ20%)\n",
    "            'test':'R:/Datasets/mosquito/sets/raw-high/test/',     ## using test folder\n",
    "            'eval':'R:/Datasets/mosquito/sets/raw-low/test/',\n",
    "\n",
    "        }\n",
    "    Data = CustomDataset(dataroot=data_dir, batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=True, input_size=options['input_size'])\n",
    "\n",
    "    ## Initialize the dataloader\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, Data.out_loaders\n",
    "\n",
    "else: ## for CIFAR100 dataset\n",
    "    Data = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'], _eval=options['eval'])\n",
    "    OODData = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['main_aug'])\n",
    "    trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
    "\n",
    "\n",
    "num_classes = Data.num_classes\n",
    "\n",
    "## modify the print frequency based on the trainloader\n",
    "options['print_freq'] = int(len(trainloader)/(options['actual_print']))\n",
    "\n",
    "print(\"Creating model: {}\".format(options['model']))\n",
    "if 'wide_resnet' in options['model']:\n",
    "    print('wide_resnet')\n",
    "    from model.wide_resnet import WideResNet\n",
    "    net = WideResNet(40, num_classes, 2, 0.0)\n",
    "elif 'allconv' in options['model']:\n",
    "    print('allconv')\n",
    "    from model.allconv import AllConvNet\n",
    "    net = AllConvNet(num_classes)\n",
    "elif 'densenet' in options['model']:\n",
    "    print('densenet')\n",
    "    from model.densenet import  densenet\n",
    "    net = densenet(num_classes=num_classes)\n",
    "elif 'resnext' in options['model']:\n",
    "    print('resnext29')\n",
    "    from model.resnext import resnext29\n",
    "    net = resnext29(num_classes)\n",
    "else:\n",
    "    print('resnet18')\n",
    "    from model.resnet import ResNet18\n",
    "    net = ResNet18(num_classes=num_classes)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "if use_gpu:\n",
    "    net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['main_aug'])\n",
    "\n",
    "if options['load_network']:\n",
    "    ## reload last saved network\n",
    "    net.load_state_dict(torch.load(load_network_adr))\n",
    "    criterion.load_state_dict(torch.load(load_criterion_adr))\n",
    "\n",
    "\n",
    "params_list = [{'params': net.parameters()},\n",
    "            {'params': criterion.parameters()}]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_acc, best_acc_out = 0.0, 0.0\n",
    "for epoch in range(options['max_epoch']):\n",
    "    print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
    "\n",
    "    train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
    "\n",
    "    if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch']:\n",
    "        print(\"==> Test\")\n",
    "        results = test_two_datasets(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
    "\n",
    "        if best_acc < results['ACC']:\n",
    "            best_acc = results['ACC']\n",
    "            print(\"Best Test Set Acc (%): {:.3f}\\t\".format(best_acc))\n",
    "            ## save the parameters for the best acc\n",
    "            save_networks(net, options['outf'], file_name, loss='BestAcc', criterion=criterion)\n",
    "\n",
    "        if best_acc_out < results['ACC_OUT']:\n",
    "            best_acc_out = results['ACC_OUT']\n",
    "            print(\"Best Out Set Acc (%): {:.3f}\\t\".format(best_acc))\n",
    "            ## save the parameters for the best acc out\n",
    "            save_networks(net, options['outf'], file_name, loss='BestAccOut', criterion=criterion)\n",
    "        \n",
    "        save_networks(net, options['outf'], file_name, loss='LastEpoch', criterion=criterion)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "elapsed = round(time.time() - start_time)\n",
    "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apr-kornia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
